{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "CONSUMER_KEY = \"eTG832aIi0pyU6EjfoTGge6gx\"\n",
    "CONSUMER_SECRET = \"mjvQFaovxMuRJ0U5XpdYfPsmmqiaZFNsL7g4qkk942OdDip6hG\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"780018504787488769-KVrRy17YTgqcUzrM32ykuYipvptNKvB\"\n",
    "ACCESS_TOKEN_SECRET = \"qtZmbBAiOrzaBxwGNdlP0bfVA3cz2AWmsv0kq1jfdIOEb\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET,\n",
    "                  sleep_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test_submition.csv\")\n",
    "test.cls = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_twts (u_id):\n",
    "    kaggle_date = 1474243200\n",
    "    timeline = []\n",
    "    timeline = api.GetUserTimeline(user_id=u_id, count=200, max_id=777880466351357952 , include_rts=True, exclude_replies=False)\n",
    "#    for i in xrange(20):\n",
    "#        if len(timeline) == 0: return []\n",
    "#        oldest_twt_id = timeline.pop().id\n",
    "#        timeline += api.GetUserTimeline(user_id=u_id, max_id=oldest_twt_id, count=200, include_rts=False, exclude_replies=True) \n",
    "#        if timeline[-1].created_at_in_seconds < kaggle_date:\n",
    "#            print \"timeline get iterations:\", i\n",
    "#            break\n",
    "#    oldest_twt_id = timeline.pop().id\n",
    "#    timeline += api.GetUserTimeline(user_id=u_id, max_id=oldest_twt_id, count=200, include_rts=False, exclude_replies=True) \n",
    "#    if timeline[-1].created_at_in_seconds > kaggle_date: \n",
    "#        print \"BAD_USER\"\n",
    "#        bad_users.add(u_id)\n",
    "    return timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = nltk.RegexpTokenizer('[a-z]+')\n",
    "\n",
    "def tokenize_twt(twt):\n",
    "    text = twt.text\n",
    "    #normalization\n",
    "    text = ''.join(c for c in unicodedata.normalize('NFD', text.lower()) if not unicodedata.combining(c))\n",
    "    #deleting URL's\n",
    "    text = re.sub('https?://\\S+', '', text)\n",
    "    #deleting HTML symbols\n",
    "    text = re.sub('&\\w+;', '', text)    \n",
    "\n",
    "    words = tokenizer.tokenize(text)\n",
    "    tokens = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def get_user_vocab(twts):\n",
    "    vocab = {}\n",
    "    for twt in twts:\n",
    "        tokens = tokenize_twt(twt)\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                vocab[token] += 1\n",
    "            else:\n",
    "                vocab[token] = 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_tokens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_users = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloaded_users = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_downloaded_users = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 7670/11947 [00:01<00:00, 4523.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4065131953 199 downloaded 594 words in vocab\n",
      "error\n",
      "28992682 0 downloaded 0 words in vocab\n",
      "1539677432 200 downloaded 1116 words in vocab\n",
      "error\n",
      "2581586548 0 downloaded 0 words in vocab\n",
      "768294420 198 downloaded 788 words in vocab\n",
      "1278705324 200 downloaded 973 words in vocab\n",
      "21567649 0 downloaded 0 words in vocab\n",
      "508080349 200 downloaded 1055 words in vocab\n",
      "469344128 0 downloaded 0 words in vocab\n",
      "162106726 198 downloaded 841 words in vocab\n",
      "3053986571 200 downloaded 948 words in vocab\n",
      "3269434189 200 downloaded 523 words in vocab\n",
      "2788368130 161 downloaded 637 words in vocab\n",
      "2652732326 197 downloaded 941 words in vocab\n",
      "549308820 0 downloaded 0 words in vocab\n",
      "3023290870 199 downloaded 904 words in vocab\n",
      "3071603159 200 downloaded 769 words in vocab\n",
      "263348660 199 downloaded 937 words in vocab\n",
      "2376564780 200 downloaded 704 words in vocab\n",
      "1516587925 200 downloaded 845 words in vocab\n",
      "error\n",
      "712671618 0 downloaded 0 words in vocab\n",
      "error\n",
      "731995571855101952 0 downloaded 0 words in vocab\n",
      "775023583 193 downloaded 755 words in vocab\n",
      "error\n",
      "2881534905 0 downloaded 0 words in vocab\n",
      "4614204614 0 downloaded 0 words in vocab\n",
      "3185303798 0 downloaded 0 words in vocab\n",
      "21955296 200 downloaded 745 words in vocab\n",
      "87641130 0 downloaded 0 words in vocab\n",
      "2966360739 200 downloaded 959 words in vocab\n",
      "16222346 200 downloaded 975 words in vocab\n",
      "50343384 200 downloaded 932 words in vocab\n",
      "724921214 191 downloaded 904 words in vocab\n",
      "36350694 0 downloaded 0 words in vocab\n",
      "37113369 196 downloaded 1230 words in vocab\n",
      "758009417111576577 0 downloaded 0 words in vocab\n",
      "758159125763194880 200 downloaded 852 words in vocab\n",
      "768736903 0 downloaded 0 words in vocab\n",
      "2774930748 200 downloaded 731 words in vocab\n",
      "3241664077 0 downloaded 0 words in vocab\n",
      "4729313955 199 downloaded 726 words in vocab\n",
      "270219231 199 downloaded 680 words in vocab\n",
      "error\n",
      "3036576329 0 downloaded 0 words in vocab\n",
      "488845496 200 downloaded 538 words in vocab\n",
      "14615775 200 downloaded 713 words in vocab\n",
      "278633892 0 downloaded 0 words in vocab\n",
      "2815890030 200 downloaded 1161 words in vocab\n",
      "147218075 200 downloaded 1034 words in vocab\n",
      "245949874 200 downloaded 894 words in vocab\n",
      "622857704 0 downloaded 0 words in vocab\n",
      "35252232 200 downloaded 922 words in vocab\n",
      "2593517424 199 downloaded 587 words in vocab\n",
      "805334443 200 downloaded 321 words in vocab\n",
      "500421445 200 downloaded 852 words in vocab\n",
      "738115099437629441 199 downloaded 734 words in vocab\n",
      "116451098 0 downloaded 0 words in vocab\n",
      "1544834228 200 downloaded 831 words in vocab\n",
      "777099913 197 downloaded 669 words in vocab\n",
      "22576799 200 downloaded 1023 words in vocab\n",
      "17693981 199 downloaded 1118 words in vocab\n",
      "156606906 0 downloaded 0 words in vocab\n",
      "75296139 199 downloaded 855 words in vocab\n",
      "451032665 200 downloaded 1010 words in vocab\n",
      "404873477 200 downloaded 1047 words in vocab\n",
      "1215478776 200 downloaded 1311 words in vocab\n",
      "4043206780 199 downloaded 1131 words in vocab\n",
      "632943628 193 downloaded 664 words in vocab\n",
      "error\n",
      "2849771796 0 downloaded 0 words in vocab\n",
      "723744795676545024 200 downloaded 477 words in vocab\n",
      "1159926900 176 downloaded 621 words in vocab\n",
      "3244052518 200 downloaded 814 words in vocab\n",
      "754380908359196672 118 downloaded 679 words in vocab\n",
      "162393428 197 downloaded 844 words in vocab\n",
      "39019743 175 downloaded 1025 words in vocab\n",
      "765003986822897668 198 downloaded 853 words in vocab\n",
      "312259327 200 downloaded 757 words in vocab\n"
     ]
    }
   ],
   "source": [
    "from twitter import TwitterError\n",
    "\n",
    "user_ids = pd.concat([train, test])['uid']\n",
    "for uid in tqdm(user_ids):\n",
    "    if uid in downloaded_users: continue\n",
    "    if len(user_tokens) >= 1000:\n",
    "        print 'TRYING TO SAVE PICKLE'\n",
    "        with open('user_tokens_'+str(len(downloaded_users))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(user_tokens, f, pickle.HIGHEST_PROTOCOL)\n",
    "            print \"pickle dumped\", 'user_tokens_'+str(len(downloaded_users))+'.pkl'\n",
    "            user_tokens = {}\n",
    "    try:    \n",
    "        twts = get_user_twts(uid)\n",
    "    except TwitterError:\n",
    "        twts = []\n",
    "        print 'error'\n",
    "        next\n",
    "    vocab = get_user_vocab(twts)\n",
    "    user_tokens[uid] = vocab\n",
    "    downloaded_users.add(uid)\n",
    "    new_downloaded_users.add(uid)\n",
    "    print uid, len(twts), 'downloaded', len(vocab), 'words in vocab'\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'twitter' from '/home/dima/anaconda2/lib/python2.7/site-packages/twitter/__init__.pyc'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload (twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5674"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(downloaded_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3393"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_downloaded_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def save_downloaded_users_set():\n",
    "    t = int(time.time())\n",
    "    with open(str(t)+'_downloaded_users.pkl', 'wb') as f:\n",
    "        pickle.dump(downloaded_users, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print \"pickle dumped\", str(t)+'_downloaded_users.pkl'\n",
    "    return\n",
    "\n",
    "def save_bad_users_set():\n",
    "    t = int(time.time())\n",
    "    with open(str(t)+'_bad_users.pkl', 'wb') as f:\n",
    "        pickle.dump(bad_users, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print \"pickle dumped\", str(t)+'_bad_users.pkl'\n",
    "    return\n",
    "\n",
    "def save_user_tokens():\n",
    "    t = int(time.time())\n",
    "    with open(str(t)+'_user_tokens.pkl', 'wb') as f:\n",
    "        pickle.dump(user_tokens, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print \"pickle dumped\", str(t)+'_user_tokens.pkl'\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle dumped 1482080100_downloaded_users.pkl\n",
      "pickle dumped 1482080101_bad_users.pkl\n",
      "pickle dumped 1482080101_user_tokens.pkl\n"
     ]
    }
   ],
   "source": [
    "save_downloaded_users_set()\n",
    "save_bad_users_set()\n",
    "save_user_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_pickle(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load test\n",
      "downloaded 1629\n",
      "bad 338\n",
      "tokens 629\n"
     ]
    }
   ],
   "source": [
    "print 'load test'\n",
    "\n",
    "print 'downloaded', len(load_pickle('1482080100_downloaded_users.pkl'))\n",
    "print 'bad', len(load_pickle('1482080101_bad_users.pkl'))\n",
    "print 'tokens', len(load_pickle('1482080101_user_tokens.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482080100_downloaded_users.pkl  test.csv\r\n",
      "1482080101_bad_users.pkl\t test_submition.csv\r\n",
      "1482080101_user_tokens.pkl\t train.csv\r\n",
      "Project. Data Preraring.ipynb\t user_tokens_1.pkl\r\n"
     ]
    }
   ],
   "source": [
    "user_tokens = load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
